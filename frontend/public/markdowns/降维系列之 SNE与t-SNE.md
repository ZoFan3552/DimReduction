# 降维系列之 SNE与t-SNE

t-SNE是一种经典的降维和可视化方法，是基于SNE（Stochastic Neighbor Embedding，随机近邻嵌入）做的，要了解t-SNE就要先了解SNE。本文同样既是总结，又是读论文笔记。

## SNE 随机近邻嵌入

SNE的第一步是用条件概率来表示高维空间中样本点之间用欧氏距离度量的相似度。假设样本选择其近邻的概率与以自身为中心的高斯分布的概率密度成正比，SNE用p_{j|i}来表示高维空间样本$x_i$会选择$x_j$作为其近邻的概率，二范数距离越近，p_{j|i}越大，二范数距离越远，p_{j|i}越小。回忆一下高斯分布的概率密度函数：

$$
f(x)=\frac{1}{\sqrt{2 \pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$



$p_{j|i}$定义为：
$$
p_{j|i}=\frac{\exp(- \|x_i-x_j \|^2/2\sigma_i^2)}{\sum_{k\neq i}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}
$$

其中$\sigma_i^2$是以$x_i$为中心的高斯分布的方差，其取值后面再给出。由于只对成对样本的相似性感兴趣，SNE令$p_{i|i}=0$。

对于$x_i,x_j$对应的低位表示$y_i,y_j$，用$q_{j|i}$表示含义相似的条件概率，不同的是这里方差取$1/\sqrt{2}$，因此：

$$
q_{j|i}=\frac{\exp(- \|y_i-y_j \|^2)}{\sum_{k\neq i}\exp(-\|y_i-y_k\|^2)}
$$

同样令$q_{j|i}=0$。如果$y_i,y_j$能够正确地表征高维样本$x_i,x_j$之间的相似性，$p_{j|i},q_{j|i}$就应该相等，因此SNE的目标就是寻找恰当的高维样本的低维表示使得$p_{j|i},q_{j|i}$之间的比例失当最小。损失函数$C$用Kullback-Leibler divergence（K-L散度，互熵，文中说此处的kl熵等价于交叉熵加了一个参数）来表示：

$$
C=\sum_i KL(P_i||Q_i)=\sum_i\sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}} \tag{1}
$$

然后SNE用梯度下降法来求解。由于K-L散度不是对称的，$C(p\|q)\neq C(q\|p)$，因此低维映射中成对的距离的不同类型误差不是以同一权重度量的。从$C$的定义来看，$p_{j|i}$较大时，期望寻找的$q_{j|i}$也是较大的，$p_{j|i}$较小时则无所谓。这说明SNE关注的主要还是如何保持局部结构（如果方差$\sigma_i^2$取值合适的话）。

接下来的问题就是设置方差值$\sigma_i^2$。SNE认为，在样本点分布比较稠密的区域，应该选择更小的方差值，否则反之。SNE设置了一个由用户调节的参数$Perp(P_i)$来衡量混乱度，其定义为：

$$
Perp(P_i)=2^{H(P_i)} \\
H(P_i)=-\sum_j p_{j|i}\log_2p_{j|i}
$$

$H(P_i)$是$P_i$以二进制度量的香农熵。这是什么意思呢，我感觉意思就是由用户给定一个$Perp(P_i)$值，然后SNE给出了$Perp(P_i)$与$p_{j|i}$的关系，根据这个关系，用二分查找的方法来确定$\sigma_i^2$。

混乱度参数可以平滑地度量有效的近邻数量，典型取值范围在5到50之间，从实验效果来看，SNE对混乱度参数的取值比较鲁棒。

SNE最小化公式$(1)$的过程是用梯度下降法来实现的，其导数惊人的简单：

$$
\frac{\partial C}{\partial y_i}=2\sum_j(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})(y_i-y_j)
$$

## 物理视角下的SNE梯度解释

从物理角度来看，这个梯度可以理解为$y_i$和所有其他$y_j$之间存在的"弹簧力"共同构成的合力。所有的弹簧力构成了沿着$(y_i-y_j)$方向的合力。$y_i$和$y_j$之间的弹簧力是排斥还是吸引，取决于二者之间的距离是否过大或过小地表示了$x_i$和$x_j$之间的原始距离。

### 优化策略

为了避免陷入局部最优解，SNE在梯度下降中添加了一个较大的动量项。具体来说，当前梯度会加上一个指数衰减的前期梯度：

$$
\mathcal{Y}^{(t)} = \mathcal{Y}^{(t-1)} + \eta \frac{\partial C}{\partial \mathcal{Y}} + \alpha(t)(\mathcal{Y}^{(t-1)} - \mathcal{Y}^{(t-2)})
$$

其中：
- $\mathcal{Y}^{(t)}$表示第$t$次迭代的解
- $\eta$是学习率
- $\alpha(t)$是第$t$次迭代的动量系数

### 噪声退火策略

在优化初期，每次迭代都会给样本点添加高斯噪声，这起到了模拟退火的效果，有助于避免陷入局部最优。如果噪声的方差变化足够缓慢，SNE就能找到更好的全局最优解。

然而，这种方法对噪声的数量和衰减速率非常敏感，通常需要在一个数据集上多次训练才能找到合适的参数。从这个角度看，SNE不如那些可以遵循凸优化原则的方法。这促使了t-SNE的发展，它不需要额外的计算和参数调整。

### 与神经网络的相似性

Hinton提出的这个SNE方法与神经网络的许多做法非常相似。论文中对SNE的批评也类似于对大多数神经网络的常见批评。这些方法的发展路径似乎正在逐渐向深度学习的方向靠拢。

## 对称SNE

t-SNE（t-Distributed Stochastic Neighbor Embedding，t分布的SNE）旨在优化SNE的上述缺点。与SNE相比，t-SNE具有以下改进：

1. 损失函数是对称的，且梯度计算更简单
2. 在低维空间使用t分布（Student-t distribution）而非高斯分布来表示样本相似性
3. 通过重尾分布（heavy-tailed distribution）缓解SNE的问题

### 损失函数定义

保持损失函数$C$的定义不变：

$$
C = \sum_i KL(P_i||Q_i) = \sum_i\sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}
$$

其中令$p_{ii} = q_{ii} = 0$。t-SNE要求$p_{ij} = p_{ji}$和$q_{ij} = q_{ji}$（对称性），因此定义：

$$
p_{ij} = \frac{\exp(-\|x_i-x_j\|^2/2\sigma^2)}{\sum_{k \neq l} \exp(-\|x_k-x_l\|^2/2\sigma^2)} \\
q_{ij} = \frac{\exp(-\|y_i-y_j\|^2)}{\sum_{k \neq l} \exp(-\|y_k-y_l\|^2)}
$$

### 处理异常值问题

上述定义存在一个问题：如果$x_i$是异常值（outlier），即所有其他样本与它的距离都很远，那么$p_{ij}$会很小，导致$x_i$对$C$的影响很小，$y_i$的位置不能很好地被其他样本影响。

解决方案是重新定义：

$$
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
$$

这样对于所有样本点$x_i$都保证$\sum_j p_{ij} > \frac{1}{2n}$，确保每个样本点都能对损失函数$C$产生显著影响。

### 简化后的梯度公式

改进后的梯度计算公式更加简洁：

$$
\frac{\partial C}{\partial y_i} = 4\sum_j (p_{ij} - q_{ij})(y_i - y_j)
$$

这种对称SNE方法不仅解决了异常值问题，还简化了梯度计算，使得优化过程更加稳定高效。

### Crowding Problem 的提出

文章提出了一个开创性的观点："crowding problem"（拥挤问题）：
- 在高维空间中，如果许多点均匀分布在样本$i$周围
- 当将这些点映射到二维空间时，二维空间中能容纳适度远点的区域比能容纳临近点的区域小得多
- 因此，要在二维空间准确表示较小的距离，就必须将样本$i$周围适当距离的点放在二维空间更远的位置

# t-SNE 的核心改进

t-SNE 采用**单自由度 Student t 分布**（等同于柯西分布）来定义$q_{ij}$：

$$
q_{ij} = \frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k \neq l} (1+\|y_k-y_l\|^2)^{-1}}
$$

#### 关键优势：
1. **对距离尺度变化不敏感**：使联合概率表示几乎不受高维和低维空间距离尺度变化的影响
2. **处理分散点集**：大的、分散的点集会像独立点一样相互作用
3. **计算效率**：相比高斯分布没有指数项，计算更方便

#### 理论支持：
- Student t 分布是高斯分布的无穷混合，与高斯分布非常接近
- 但相比高斯分布能更好地解决 crowding problem

### 优化过程

优化过程与 SNE 和对称 SNE 相同，但尺度更合适。梯度计算公式为：

$$
\frac{\partial C}{\partial y_i} = 4\sum_j (p_{ij}-q_{ij})(y_i-y_j)(1+\|y_i-y_j\|^2)^{-1}
$$

这个梯度公式：
- 保持了对称 SNE 的简洁性
- 通过$(1+\|y_i-y_j\|^2)^{-1}$项更好地处理不同尺度下的距离关系
- 使得优化过程更加稳定和高效